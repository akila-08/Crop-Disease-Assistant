{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ViT expects 224x224\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalCropDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, scaler=None):\n",
    "        self.df = df.copy()\n",
    "        self.transform = transform\n",
    "        self.scaler = scaler if scaler else StandardScaler()\n",
    "        \n",
    "        # Standardize numerical features (excluding 'image_path' and 'Label')\n",
    "        numerical_cols = self.df.columns[:-2]  # Assuming last two are 'image_path' and 'Label'\n",
    "        if scaler is None:\n",
    "            self.df[numerical_cols] = self.scaler.fit_transform(self.df[numerical_cols])\n",
    "        else:\n",
    "            self.df[numerical_cols] = self.scaler.transform(self.df[numerical_cols])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx][\"image_path\"]\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        numerical_features = pd.to_numeric(self.df.iloc[idx, :-2], errors='coerce').values.astype('float32')\n",
    "        numerical_features = torch.tensor(numerical_features, dtype=torch.float32)\n",
    "        label = torch.tensor(self.df.iloc[idx][\"Label\"], dtype=torch.long)\n",
    "        return image, numerical_features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalViT(nn.Module):\n",
    "    def __init__(self, num_numerical_features, num_classes, dropout_rate=0.3):\n",
    "        super(MultimodalViT, self).__init__()\n",
    "        \n",
    "        # Pretrained Vision Transformer\n",
    "        self.vit = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n",
    "        self.vit.heads = nn.Identity()  # Remove default classification head\n",
    "        \n",
    "        # Numerical feature encoder\n",
    "        self.num_encoder = nn.Sequential(\n",
    "            nn.Linear(num_numerical_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Cross-attention layer\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=768, num_heads=8)  # ViT output dim is 768\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, 768))  # Learnable classification token\n",
    "        \n",
    "        # Classification head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, image, numerical_features):\n",
    "        # Extract image features from ViT\n",
    "        img_features = self.vit(image)  # Shape: (batch_size, 768)\n",
    "        img_features = img_features.unsqueeze(0)  # Shape: (1, batch_size, 768)\n",
    "        \n",
    "        # Process numerical features\n",
    "        num_features = self.num_encoder(numerical_features)  # Shape: (batch_size, 128)\n",
    "        num_features = num_features.unsqueeze(0)  # Shape: (1, batch_size, 128)\n",
    "        \n",
    "        # Pad numerical features to match ViT embedding size\n",
    "        num_features = nn.functional.pad(num_features, (0, 768 - 128))  # Shape: (1, batch_size, 768)\n",
    "        \n",
    "        # Add CLS token for classification\n",
    "        batch_size = image.size(0)\n",
    "        cls_tokens = self.cls_token.expand(-1, batch_size, -1)  # Shape: (1, batch_size, 768)\n",
    "        \n",
    "        # Combine features for cross-attention\n",
    "        combined_features = torch.cat((cls_tokens, img_features, num_features), dim=0)  # Shape: (3, batch_size, 768)\n",
    "        \n",
    "        # Apply cross-attention\n",
    "        attn_output, _ = self.cross_attention(cls_tokens, combined_features, combined_features)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.fc(attn_output.squeeze(0))  # Shape: (batch_size, num_classes)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = r\"C:\\Users\\Samridhaa\\OneDrive\\Desktop\\New_DL\\mapped_data_with_images.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Label\"] = label_encoder.fit_transform(df[\"Label\"])\n",
    "num_classes = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 1760\n",
      "Test set size: 440\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"Label\"], random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MultimodalCropDataset(train_df, transform=transform)\n",
    "test_dataset = MultimodalCropDataset(test_df, transform=transform, scaler=train_dataset.scaler)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"Train set size: {len(train_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Should print True if GPU is detected\n",
    "print(torch.cuda.current_device())  # Should print 0 (or device index)\n",
    "print(torch.cuda.get_device_name(0))  # Should print your GPU name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultimodalViT(num_numerical_features=7, num_classes=num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for single sample: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start_time = time.time()\n",
    "sample = train_dataset[0]  # Change this to a random index if needed\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken for single sample: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([16, 3, 224, 224])\n",
      "Numerical features shape: torch.Size([16, 7])\n",
      "Labels shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# Check sample batch\n",
    "images, num_features, labels = next(iter(train_loader))\n",
    "print(f\"Image batch shape: {images.shape}\")\n",
    "print(f\"Numerical features shape: {num_features.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 0.4946\n",
      "Epoch [2/15], Loss: 0.0532\n",
      "Epoch [3/15], Loss: 0.0232\n",
      "Epoch [4/15], Loss: 0.1023\n",
      "Epoch [5/15], Loss: 0.0718\n",
      "Epoch [6/15], Loss: 0.0408\n",
      "Epoch [7/15], Loss: 0.0124\n",
      "Epoch [8/15], Loss: 0.0305\n",
      "Epoch [9/15], Loss: 0.0070\n",
      "Epoch [10/15], Loss: 0.0050\n",
      "Epoch [11/15], Loss: 0.0042\n",
      "Epoch [12/15], Loss: 0.0039\n",
      "Epoch [13/15], Loss: 0.0034\n",
      "Epoch [14/15], Loss: 0.0030\n",
      "Epoch [15/15], Loss: 0.0027\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, num_features, labels in train_loader:\n",
    "        images, num_features, labels = images.to(device), num_features.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, num_features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    train_loss = train_loss / len(train_dataset)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      2\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m      3\u001b[0m preds, targets \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "preds, targets = [], []\n",
    "with torch.no_grad():\n",
    "    for images, num_features, labels in test_loader:\n",
    "        images, num_features, labels = images.to(device), num_features.to(device), labels.to(device)\n",
    "        outputs = model(images, num_features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item() * images.size(0)\n",
    "            \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        preds.extend(predicted.cpu().numpy())\n",
    "        targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "val_loss = val_loss / len(test_dataset)\n",
    "val_acc = accuracy_score(targets, preds)\n",
    "    \n",
    "print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "if val_acc > best_acc:\n",
    "    best_acc = val_acc\n",
    "    torch.save(model.state_dict(), \"vit_multimodal_best.pth\")\n",
    "    print(f\"Saved best model with accuracy: {best_acc:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_vit(image_path, numerical_features=None, model_path=\"vit_multimodal_best.pth\"):\n",
    "    # Load model\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Process image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = Image.fromarray(image)\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Process numerical features\n",
    "    if numerical_features is None:\n",
    "        numerical_features = np.zeros(7)  # Default if not provided\n",
    "    numerical_features = train_dataset.scaler.transform(numerical_features.reshape(1, -1))[0]\n",
    "    numerical_features = torch.tensor(numerical_features, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        output = model(image, numerical_features)\n",
    "        predicted_class = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    # Map back to class name\n",
    "    index_to_class = {v: k for k, v in dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))).items()}\n",
    "    return index_to_class[predicted_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samridhaa\\AppData\\Local\\Temp\\ipykernel_15432\\3530710391.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
      "c:\\Users\\Samridhaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Samridhaa\\AppData\\Local\\Temp\\ipykernel_15432\\3530710391.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (without numerical features): Apple___Cedar_apple_rust\n",
      "Prediction (with numerical features): Apple___Cedar_apple_rust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Samridhaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "image_path = r\"C:\\Users\\Samridhaa\\OneDrive\\Desktop\\New_DL\\Test\\0a5e9323-dbad-432d-ac58-d291718345d9___FREC_Scab 3417.JPG\"\n",
    "try:\n",
    "    # Example with dummy numerical features\n",
    "    result = predict_vit(image_path)\n",
    "    print(f\"Prediction (without numerical features): {result}\")\n",
    "    \n",
    "    # Example with sample numerical features\n",
    "    sample_num_features = np.array([0.5, 1.2, 0.8, 2.1, 0.9, 1.5, 0.3])  # Replace with real data if available\n",
    "    result = predict_vit(image_path, sample_num_features)\n",
    "    print(f\"Prediction (with numerical features): {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during inference: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Apple___Apple_scab', 1: 'Apple___Black_rot', 2: 'Apple___Cedar_apple_rust', 3: 'Apple___healthy', 4: 'Blueberry___healthy', 5: 'Cherry_(including_sour)___Powdery_mildew', 6: 'Cherry_(including_sour)___healthy', 7: 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 8: 'Corn_(maize)___Common_rust_', 9: 'Corn_(maize)___Northern_Leaf_Blight', 10: 'Corn_(maize)___healthy', 11: 'Grape___Black_rot', 12: 'Grape___Esca_(Black_Measles)', 13: 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 14: 'Grape___healthy', 15: 'Orange___Haunglongbing_(Citrus_greening)', 16: 'Peach___Bacterial_spot', 17: 'Peach___healthy', 18: 'Pepper,_bell___Bacterial_spot', 19: 'Pepper,_bell___healthy', 20: 'Potato___Early_blight', 21: 'Potato___healthy'}\n"
     ]
    }
   ],
   "source": [
    "class_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"vit_multimodal_final.pth\")\n",
    "print(\"Final model saved as 'vit_multimodal_final.pth'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
